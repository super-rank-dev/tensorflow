diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
--- stablehlo/BUILD.bazel
+++ stablehlo/BUILD.bazel
@@ -370,6 +370,7 @@
         ":reference_tensor",
         ":reference_types",
         ":stablehlo_ops",
+        ":stablehlo_type_inference",
         "@llvm-project//llvm:Support",
         "@llvm-project//mlir:FuncDialect",
         "@llvm-project//mlir:IR",
diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.cpp b/stablehlo/stablehlo/dialect/TypeInference.cpp
--- stablehlo/stablehlo/dialect/TypeInference.cpp
+++ stablehlo/stablehlo/dialect/TypeInference.cpp
@@ -376,8 +376,8 @@
 //            padding_high,
 //                         dilatedBound(window_shape[d]))
 //      where (padding_low, padding_high) is the padding-pair for d.
-SmallVector<int64_t> inferWindowOutputShape(
-    const ArrayRef<int64_t> baseShape, const ArrayRef<WindowDimension> window) {
+SmallVector<int64_t> inferWindowOutputShape(ArrayRef<int64_t> baseShape,
+                                            ArrayRef<WindowDimension> window) {
   assert(baseShape.size() == window.size() &&
          "Size of window dimensions must match the size of base shape.");
 
@@ -652,7 +652,7 @@
     SmallVector<int64_t>& windowDims,
     SmallVector<WindowDimension>& inferredWindow) {
   // reduce_window_c1
-  if (inputArgTypes.size() < 1)
+  if (inputArgTypes.empty())
     return emitOptionalError(location, "requires at least 1 input value");
 
   // Check for unranked tensors in input operands.
diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.h b/stablehlo/stablehlo/dialect/TypeInference.h
--- stablehlo/stablehlo/dialect/TypeInference.h
+++ stablehlo/stablehlo/dialect/TypeInference.h
@@ -76,8 +76,8 @@
     ArrayRef<int64_t> lhsDilation, ArrayRef<int64_t> rhsDilation,
     ArrayRef<bool> windowReversal, std::optional<Location> loc);
 
-SmallVector<int64_t> inferWindowOutputShape(
-    const ArrayRef<int64_t> baseShape, const ArrayRef<WindowDimension> window);
+SmallVector<int64_t> inferWindowOutputShape(ArrayRef<int64_t> baseShape,
+                                            ArrayRef<WindowDimension> window);
 
 LogicalResult verifyReplicaGroups(std::optional<Location> location,
                                   DenseIntElementsAttr replicaGroups,
@@ -297,8 +297,8 @@
                                SmallVectorImpl<Type>& inferredReturnTypes);
 
 LogicalResult inferReverseOp(
-    std::optional<Location> location, Type operands,
-    SmallVectorImpl<ShapedTypeComponents>& inferredReturnTypes);
+    std::optional<Location> location, Type operandType,
+    SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes);
 
 LogicalResult inferRngOp(
     std::optional<Location> location, Value a, Value b, Value shape,
diff --ruN a/stablehlo/stablehlo/integrations/python/StablehloModule.cpp b/stablehlo/stablehlo/integrations/python/StablehloModule.cpp
--- stablehlo/stablehlo/integrations/python/StablehloModule.cpp
+++ stablehlo/stablehlo/integrations/python/StablehloModule.cpp
@@ -18,7 +18,6 @@
 #include "stablehlo/integrations/c/StablehloAttributes.h"
 #include "stablehlo/integrations/c/StablehloDialect.h"
 #include "stablehlo/integrations/c/StablehloTypes.h"
-#include "stablehlo/integrations/python/PortableApi.h"
 
 namespace py = pybind11;
 
@@ -481,7 +480,10 @@
   //
   // Portable APIs
   //
-  mlir::stablehlo::AddPortableApi(m);
+  // TODO(b/282235236): Replace with `stablehlo::AddPortableApi(m)`.
+  // Cannot use AddPortableApi right away because that would break JAX.
+  m.def("get_current_version", []() { return "0.11.3"; });
+  m.def("get_minimum_version", []() { return "0.9.0"; });
 
   //
   // Serialization APIs.
diff --ruN a/stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py b/stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
--- stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
+++ stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
@@ -17,3 +17,12 @@
 # pylint: disable=wildcard-import,relative-beyond-top-level,g-import-not-at-top
 from ._stablehlo_ops_gen import *
 from .._mlir_libs._stablehlo import *
+
+
+def get_earliest_forward_compatible_version():
+  """Return the earliest StableHLO version that the current StableHLO version
+    is still forward compatible with.
+  """
+  # TODO(b/282232437): Delete this TensorFlow-only API, now that we have an OSS
+  # equivalent that does the same thing.
+  return get_minimum_version()
diff --ruN a/stablehlo/stablehlo/integrations/python/tests/stablehlo.py b/stablehlo/stablehlo/integrations/python/tests/stablehlo.py
--- stablehlo/stablehlo/integrations/python/tests/stablehlo.py
+++ stablehlo/stablehlo/integrations/python/tests/stablehlo.py
@@ -210,18 +210,22 @@
   assert type(api_version) == int
   assert api_version > 0
 
+
 def is_semver_format(version_str):
-  return re.match('^\d+\.\d+\.\d+$', version_str)
+  return re.match("^\d+\.\d+\.\d+$", version_str)
+
 
 @run
 def test_current_version():
   curr_version = stablehlo.get_current_version()
   assert is_semver_format(curr_version)
 
+
 @run
 def test_minimum_version():
   curr_version = stablehlo.get_minimum_version()
   assert is_semver_format(curr_version)
+
 
 ASM = """
 func.func @test(%arg0: tensor<2xf32>) -> tensor<2xf32> {
diff --ruN a/stablehlo/stablehlo/reference/CMakeLists.txt b/stablehlo/stablehlo/reference/CMakeLists.txt
--- stablehlo/stablehlo/reference/CMakeLists.txt
+++ stablehlo/stablehlo/reference/CMakeLists.txt
@@ -90,4 +90,5 @@
   StablehloReferenceScope
   StablehloReferenceSizes
   StablehloReferenceTensor
+  StablehloTypeInference
 )
diff --ruN a/stablehlo/stablehlo/reference/Ops.cpp b/stablehlo/stablehlo/reference/Ops.cpp
--- stablehlo/stablehlo/reference/Ops.cpp
+++ stablehlo/stablehlo/reference/Ops.cpp
@@ -62,10 +62,10 @@
                                  const Axes &dimensions, Region &body,
                                  Scope &scope) {
   SmallVector<Type> inputTypes;
-  for (auto input : inputs) inputTypes.push_back(input.getType());
+  for (const auto &input : inputs) inputTypes.push_back(input.getType());
 
   SmallVector<Type> initValueTypes;
-  for (auto initValue : initValues)
+  for (const auto &initValue : initValues)
     initValueTypes.push_back(initValue.getType());
 
   SmallVector<ShapedTypeComponents> inferredReduceTypes;
@@ -78,7 +78,7 @@
         invalidArgument("Could not infer ReduceOp's return type"));
 
   SmallVector<ShapedType> resultTypes;
-  for (auto inferredType : inferredReduceTypes) {
+  for (const auto &inferredType : inferredReduceTypes) {
     auto shapedType = hlo::createShapedType(inferredType);
     if (!shapedType)
       llvm::report_fatal_error("Could not infer ReduceOp's return type");
@@ -980,7 +980,7 @@
        resultIt != results[0].index_end(); ++resultIt) {
     SmallVector<Tensor> windows;
     auto windowStart = (*resultIt) * windowStrides;
-    for (auto paddedInput : paddedInputs)
+    for (const auto &paddedInput : paddedInputs)
       windows.push_back(evalSliceOp(paddedInput, windowStart,
                                     windowStart + windowDimensions,
                                     windowDilations));
@@ -1108,7 +1108,7 @@
                                bool isStable, Region &comparator,
                                Scope &scope) {
   SmallVector<Tensor> results;
-  for (auto input : inputs) results.push_back(Tensor(input.getType()));
+  for (const auto &input : inputs) results.push_back(Tensor(input.getType()));
   auto adjustedDimension =
       dimension >= 0 ? dimension : dimension + inputs[0].getRank();
 
diff --ruN a/stablehlo/stablehlo/reference/Tensor.cpp b/stablehlo/stablehlo/reference/Tensor.cpp
--- stablehlo/stablehlo/reference/Tensor.cpp
+++ stablehlo/stablehlo/reference/Tensor.cpp
@@ -93,11 +93,11 @@
 Tensor::Tensor(ShapedType type, AsmResourceBlob blob)
     : impl_(llvm::makeIntrusiveRefCnt<detail::Buffer>(type, std::move(blob))) {}
 
-Tensor::Tensor(ShapedType type, const Element &element)
+Tensor::Tensor(ShapedType type, const Element &initValue)
     : impl_(llvm::makeIntrusiveRefCnt<detail::Buffer>(type)) {
   for (auto indexIt = this->index_begin(); indexIt != this->index_end();
        ++indexIt)
-    this->set(*indexIt, element);
+    this->set(*indexIt, initValue);
 }
 
 Element Tensor::get(const Index &index) const {
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
@@ -19,7 +19,7 @@
 
 // -----
 
-// expected-error@-3{{must have no more than one function}}
+// expected-error@-3{{must have no more than one function or a `main` function to clearly identify which function will be refined}}
 func.func @error_too_many_functions(%arg0: tensor<f32>) -> tensor<f32> {
   %0 = func.call @helper(%arg0) : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
@@ -27,6 +27,24 @@
 
 func.func private @helper(%arg0: tensor<f32>) -> tensor<f32> {
   return %arg0 : tensor<f32>
+}
+
+// -----
+
+module @has_main {
+  // CHECK: main
+  func.func @main(%arg0: tensor<4xf32>) -> tensor<*xi32> {
+    // CHECK: stablehlo.bitcast_convert{{.*}} -> tensor<4xi32>
+    %0 = stablehlo.bitcast_convert %arg0 : (tensor<4xf32>) -> tensor<*xi32>
+    func.return %0 : tensor<*xi32>
+  }
+
+  // CHECK: helper
+  func.func @helper(%arg0: tensor<4xf32>) -> tensor<*xi32> {
+    // CHECK: stablehlo.bitcast_convert{{.*}} -> tensor<*xi32>
+    %0 = stablehlo.bitcast_convert %arg0 : (tensor<4xf32>) -> tensor<*xi32>
+    func.return %0 : tensor<*xi32>
+  }
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -1089,18 +1089,29 @@
     // Current use cases are served well by inlining multiple functions into
     // a single function, so we leave native support for multiple functions to
     // future work.
+    // To enable modules that contain CustomCallOp::called_computations,
+    // we allow multiple functions, in which case we only refine the main
+    // function called "main", assuming that the called computations will have
+    // static shapes. Lifting this assumption and expanding refinement to
+    // multiple functions is left for future work.
     ModuleOp module = getOperation();
     auto funcs = llvm::to_vector(module.getOps<func::FuncOp>());
     if (funcs.empty()) return;
-    if (funcs.size() != 1) {
-      module.emitOpError() << "must have no more than one function";
+    func::FuncOp func;
+    if (funcs.size() == 1) {
+      func = funcs[0];
+    } else {
+      func = module.lookupSymbol<func::FuncOp>("main");
+    }
+    if (!func) {
+      module.emitOpError() << "must have no more than one function or a `main`"
+          << " function to clearly identify which function will be refined";
       return signalPassFailure();
     }
 
     // Similarly, only one block per function is supported at the moment.
     // At the StableHLO level, functions are expected to only have one block,
     // so supporting more is out of scope for this pass.
-    func::FuncOp func = funcs[0];
     if (!func.getRegion().hasOneBlock()) {
       func.emitOpError() << "must have exactly one block";
       return signalPassFailure();
@@ -1155,7 +1166,7 @@
     patterns.add<RefineWhileOpPattern>(&getContext());
     patterns.add<UpdateFunctionTypePattern>(&getContext());
     patterns.add<UpdateRegionTypePattern>(&getContext());
-    if (failed(applyPatternsAndFoldGreedily(module, std::move(patterns),
+    if (failed(applyPatternsAndFoldGreedily(func, std::move(patterns),
                                             config))) {
       return signalPassFailure();
     }

